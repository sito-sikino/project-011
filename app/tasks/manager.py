"""
Task Manager for Discord Multi-Agent System

Phase 4: Task Management System å®Ÿè£…å®Œäº†
Pydanticã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã€ã‚¿ã‚¹ã‚¯CRUDæ“ä½œã€Redisçµ±åˆã€ã‚­ãƒ¥ãƒ¼ç®¡ç†ã‚’æä¾›

t-wadaå¼TDDã‚µã‚¤ã‚¯ãƒ«å®Ÿè£…ãƒ•ãƒ­ãƒ¼:
ðŸ”´ Red Phase: åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆä½œæˆå®Œäº†ï¼ˆ3ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã€100+ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼‰
ðŸŸ¢ Green Phase: æœ€å°å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆé€šéŽ
ðŸŸ¡ Refactor Phase: å“è³ªå‘ä¸Šã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–

å®Ÿè£…æ©Ÿèƒ½:
- TaskModel: Pydantic v2 ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ã€Fieldåˆ¶ç´„ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- TaskStatus/TaskPriority: Enum ã«ã‚ˆã‚‹çŠ¶æ…‹ãƒ»å„ªå…ˆåº¦ç®¡ç†
- TaskManager: éžåŒæœŸCRUDæ“ä½œã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆRedis + PostgreSQLï¼‰
- RedisTaskQueue: å„ªå…ˆåº¦ã‚­ãƒ¥ãƒ¼ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ç®¡ç†ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: Fail-Fastè¨­è¨ˆã€ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã‚¯ãƒ©ã‚¹
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æœ€é©åŒ–: Redis ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ä¸¦è¡Œå‡¦ç†å¯¾å¿œ
"""
import asyncio
import json
import redis.asyncio as redis
import logging
from datetime import datetime, timezone, timedelta
from enum import Enum
from typing import Optional, List, Dict, Any, Union
from uuid import UUID, uuid4
from contextlib import asynccontextmanager

from pydantic import BaseModel, Field, validator, model_validator
from app.core.settings import Settings, get_settings
from app.core.database import DatabaseManager, get_db_manager

logger = logging.getLogger(__name__)


# Enums for Task Status and Priority
class TaskStatus(str, Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å®šç¾©"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class TaskPriority(str, Enum):
    """ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦å®šç¾©"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


# Custom Exception Classes
class TaskError(Exception):
    """ã‚¿ã‚¹ã‚¯æ“ä½œã‚¨ãƒ©ãƒ¼ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"""
    pass


class TaskNotFoundError(TaskError):
    """ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass


class TaskValidationError(TaskError):
    """ã‚¿ã‚¹ã‚¯ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼"""
    pass


class QueueError(Exception):
    """ã‚­ãƒ¥ãƒ¼æ“ä½œã‚¨ãƒ©ãƒ¼ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"""
    pass


class QueueEmptyError(QueueError):
    """ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass


class QueueFullError(QueueError):
    """ã‚­ãƒ¥ãƒ¼ãŒæº€æ¯ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼"""
    pass


# Task Model Definition
class TaskModel(BaseModel):
    """
    Pydantic v2 ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«
    
    Phase 4.1: TaskModelå®Ÿè£…
    - UUIDè‡ªå‹•ç”Ÿæˆã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç®¡ç†
    - Fieldåˆ¶ç´„ã«ã‚ˆã‚‹å …ç‰¢ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ»å„ªå…ˆåº¦Enumç®¡ç†
    - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONBå¯¾å¿œ
    - æ¯”è¼ƒãƒ»æ›´æ–°ãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ã‚½ãƒƒãƒ‰
    """
    
    # Core Fields
    id: UUID = Field(default_factory=uuid4, description="ã‚¿ã‚¹ã‚¯IDï¼ˆUUIDï¼‰")
    title: str = Field(
        ..., 
        min_length=1, 
        max_length=200, 
        description="ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒˆãƒ«"
    )
    description: str = Field(
        ..., 
        max_length=2000, 
        description="ã‚¿ã‚¹ã‚¯èª¬æ˜Ž"
    )
    
    # Status and Priority
    status: TaskStatus = Field(
        default=TaskStatus.PENDING,
        description="ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"
    )
    priority: TaskPriority = Field(
        default=TaskPriority.MEDIUM,
        description="ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦"
    )
    
    # Optional Fields
    agent_id: Optional[str] = Field(
        None,
        max_length=100,
        description="æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆID"
    )
    channel_id: Optional[str] = Field(
        None,
        pattern=r'^\d{17,19}$',  # Discord ID format (17-19 digits)
        description="é–¢é€£Discordãƒãƒ£ãƒ³ãƒãƒ«ID"
    )
    
    # Timestamps
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc),
        description="ä½œæˆæ—¥æ™‚"
    )
    updated_at: datetime = Field(
        default=None,
        description="æ›´æ–°æ—¥æ™‚"
    )
    
    # Metadata
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONBï¼‰"
    )
    
    model_config = {
        "json_encoders": {
            datetime: lambda v: v.isoformat(),
            UUID: str
        }
    }
    
    def model_post_init(self, __context) -> None:
        """åˆæœŸåŒ–å¾Œå‡¦ç†ï¼šupdated_atã‚’created_atã¨åŒã˜å€¤ã«è¨­å®š"""
        if self.updated_at is None:
            self.updated_at = self.created_at
        
    def __eq__(self, other) -> bool:
        """ã‚¿ã‚¹ã‚¯åŒç­‰æ€§æ¯”è¼ƒï¼ˆIDãƒ™ãƒ¼ã‚¹ï¼‰"""
        if not isinstance(other, TaskModel):
            return False
        return self.id == other.id
    
    def __hash__(self) -> int:
        """ãƒãƒƒã‚·ãƒ¥å€¤ç”Ÿæˆï¼ˆIDãƒ™ãƒ¼ã‚¹ï¼‰"""
        return hash(self.id)
    
    def update_status(self, new_status: TaskStatus) -> None:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ›´æ–°"""
        self.status = new_status
        self.updated_at = datetime.now(timezone.utc)
    
    def is_completed(self) -> bool:
        """å®Œäº†çŠ¶æ…‹ç¢ºèª"""
        return self.status == TaskStatus.COMPLETED
    
    def is_active(self) -> bool:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–çŠ¶æ…‹ç¢ºèªï¼ˆé€²è¡Œä¸­ãƒ»ä¿ç•™ä¸­ï¼‰"""
        return self.status in [TaskStatus.PENDING, TaskStatus.IN_PROGRESS]
    
    def add_metadata(self, key: str, value: Any) -> None:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ›´æ–°"""
        self.metadata[key] = value
        self.updated_at = datetime.now(timezone.utc)
    
    def get_duration(self) -> timedelta:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“è¨ˆç®—"""
        return self.updated_at - self.created_at


# Task Manager Implementation  
class TaskManager:
    """
    ã‚¿ã‚¹ã‚¯CRUDæ“ä½œç®¡ç†ã‚¯ãƒ©ã‚¹
    
    Phase 4.2: TaskManagerå®Ÿè£…
    - éžåŒæœŸCRUDæ“ä½œ
    - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆRedis Hot + PostgreSQL Coldï¼‰
    - åŽŸå­çš„æ“ä½œï¼ˆRedis + PostgreSQL åŒæœŸï¼‰
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»Fail-Fastè¨­è¨ˆ
    - settings.pyçµ±åˆ
    """
    
    def __init__(self, settings: Settings):
        """
        TaskManageråˆæœŸåŒ–
        
        Args:
            settings: è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.settings = settings
        self.redis_url = settings.database.redis_url
        self.database_url = settings.database.url
        self.redis_client: Optional[redis.Redis] = None
        self.db_manager: Optional[DatabaseManager] = None
        logger.info("TaskManager initialized")
    
    async def initialize(self) -> None:
        """
        TaskManageråˆæœŸåŒ–ï¼ˆRedisãƒ»DBæŽ¥ç¶šã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼‰
        
        Raises:
            TaskError: åˆæœŸåŒ–å¤±æ•—æ™‚
        """
        try:
            # RedisæŽ¥ç¶šåˆæœŸåŒ–
            self.redis_client = redis.from_url(
                self.redis_url,
                encoding='utf-8',
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†åˆæœŸåŒ–
            self.db_manager = get_db_manager()
            await self.db_manager.initialize()
            
            logger.info("TaskManager connections initialized successfully")
            
        except Exception as e:
            error_msg = f"TaskManager initialization failed: {e}"
            logger.critical(error_msg)
            raise TaskError(error_msg) from e
    
    async def close(self) -> None:
        """
        TaskManagerçµ‚äº†ï¼ˆæŽ¥ç¶šã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
        """
        if self.redis_client:
            await self.redis_client.close()
            self.redis_client = None
        
        if self.db_manager:
            await self.db_manager.close()
            
        logger.info("TaskManager connections closed")
    
    # CREATE operations
    async def create_task(
        self,
        title: str,
        description: str,
        status: TaskStatus = TaskStatus.PENDING,
        priority: TaskPriority = TaskPriority.MEDIUM,
        agent_id: Optional[str] = None,
        channel_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> TaskModel:
        """
        ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆRedis + PostgreSQLåŒæ™‚ä¿å­˜ï¼‰
        
        Args:
            title: ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒˆãƒ«
            description: ã‚¿ã‚¹ã‚¯èª¬æ˜Ž
            status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: PENDINGï¼‰
            priority: å„ªå…ˆåº¦ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: MEDIUMï¼‰
            agent_id: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆID
            channel_id: ãƒãƒ£ãƒ³ãƒãƒ«ID
            metadata: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            TaskModel: ä½œæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯
            
        Raises:
            TaskValidationError: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—æ™‚
            TaskError: ä½œæˆå¤±æ•—æ™‚
        """
        try:
            # ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            task = TaskModel(
                title=title,
                description=description,
                status=status,
                priority=priority,
                agent_id=agent_id,
                channel_id=channel_id,
                metadata=metadata or {}
            )
            
            # Redisä¿å­˜
            await self._save_task_to_redis(task)
            
            # PostgreSQLä¿å­˜
            await self._save_task_to_database(task)
            
            logger.info(f"Task created successfully: {task.id}")
            return task
            
        except Exception as e:
            if "validation" in str(e).lower():
                raise TaskValidationError(f"Task validation failed: {e}") from e
            else:
                raise TaskError(f"Task creation failed: {e}") from e
    
    # READ operations
    async def get_task(self, task_id: UUID) -> TaskModel:
        """
        ã‚¿ã‚¹ã‚¯å–å¾—ï¼ˆRediså„ªå…ˆã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§PostgreSQLï¼‰
        
        Args:
            task_id: ã‚¿ã‚¹ã‚¯ID
            
        Returns:
            TaskModel: å–å¾—ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯
            
        Raises:
            TaskNotFoundError: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        """
        # Redisç¢ºèª
        task = await self.get_task_from_redis(task_id)
        if task:
            return task
        
        # PostgreSQLç¢ºèª
        task = await self.get_task_from_database(task_id)
        if task:
            # Rediså†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
            await self._save_task_to_redis(task)
            return task
        
        raise TaskNotFoundError(f"Task not found: {task_id}")
    
    async def get_task_from_redis(self, task_id: UUID) -> Optional[TaskModel]:
        """Redisã‹ã‚‰ã‚¿ã‚¹ã‚¯å–å¾—"""
        try:
            task_data = await self.redis_client.hget("tasks", str(task_id))
            if task_data:
                task_dict = json.loads(task_data)
                return TaskModel(**task_dict)
            return None
        except Exception as e:
            logger.warning(f"Redis task retrieval failed: {e}")
            return None
    
    async def get_task_from_database(self, task_id: UUID) -> Optional[TaskModel]:
        """PostgreSQLã‹ã‚‰ã‚¿ã‚¹ã‚¯å–å¾—"""
        try:
            query = """
            SELECT id, title, description, status, priority, agent_id, channel_id, 
                   created_at, updated_at, metadata
            FROM tasks WHERE id = $1
            """
            rows = await self.db_manager.fetch(query, task_id)
            if rows:
                row = rows[0]
                return TaskModel(**row)
            return None
        except Exception as e:
            logger.error(f"Database task retrieval failed: {e}")
            return None
    
    async def get_tasks_by_status(self, status: TaskStatus) -> List[TaskModel]:
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥ã‚¿ã‚¹ã‚¯å–å¾—"""
        try:
            query = """
            SELECT id, title, description, status, priority, agent_id, channel_id, 
                   created_at, updated_at, metadata
            FROM tasks WHERE status = $1
            ORDER BY created_at ASC
            """
            rows = await self.db_manager.fetch(query, status.value)
            return [TaskModel(**row) for row in rows]
        except Exception as e:
            logger.error(f"Status-based task retrieval failed: {e}")
            return []
    
    async def get_tasks_by_agent(self, agent_id: str) -> List[TaskModel]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã‚¿ã‚¹ã‚¯å–å¾—"""
        try:
            query = """
            SELECT id, title, description, status, priority, agent_id, channel_id, 
                   created_at, updated_at, metadata
            FROM tasks WHERE agent_id = $1
            ORDER BY created_at ASC
            """
            rows = await self.db_manager.fetch(query, agent_id)
            return [TaskModel(**row) for row in rows]
        except Exception as e:
            logger.error(f"Agent-based task retrieval failed: {e}")
            return []
    
    async def get_tasks_by_channel(self, channel_id: str) -> List[TaskModel]:
        """ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã‚¿ã‚¹ã‚¯å–å¾—"""
        try:
            query = """
            SELECT id, title, description, status, priority, agent_id, channel_id, 
                   created_at, updated_at, metadata
            FROM tasks WHERE channel_id = $1
            ORDER BY created_at ASC
            """
            rows = await self.db_manager.fetch(query, channel_id)
            return [TaskModel(**row) for row in rows]
        except Exception as e:
            logger.error(f"Channel-based task retrieval failed: {e}")
            return []
    
    # UPDATE operations
    async def update_task(
        self,
        task_id: UUID,
        title: Optional[str] = None,
        description: Optional[str] = None,
        status: Optional[TaskStatus] = None,
        priority: Optional[TaskPriority] = None,
        agent_id: Optional[str] = None,
        channel_id: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> TaskModel:
        """
        ã‚¿ã‚¹ã‚¯æ›´æ–°ï¼ˆRedis + PostgreSQLåŒæœŸæ›´æ–°ï¼‰
        
        Args:
            task_id: æ›´æ–°å¯¾è±¡ã‚¿ã‚¹ã‚¯ID
            title: æ–°ã—ã„ã‚¿ã‚¤ãƒˆãƒ«
            description: æ–°ã—ã„èª¬æ˜Ž
            status: æ–°ã—ã„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            priority: æ–°ã—ã„å„ªå…ˆåº¦
            agent_id: æ–°ã—ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆID
            channel_id: æ–°ã—ã„ãƒãƒ£ãƒ³ãƒãƒ«ID
            metadata: æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            TaskModel: æ›´æ–°å¾Œã®ã‚¿ã‚¹ã‚¯
            
        Raises:
            TaskNotFoundError: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
            TaskValidationError: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—æ™‚
        """
        # æ—¢å­˜ã‚¿ã‚¹ã‚¯å–å¾—
        task = await self.get_task(task_id)
        
        # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰æ›´æ–°
        update_data = {}
        if title is not None:
            update_data["title"] = title
        if description is not None:
            update_data["description"] = description
        if status is not None:
            update_data["status"] = status
        if priority is not None:
            update_data["priority"] = priority
        if agent_id is not None:
            update_data["agent_id"] = agent_id
        if channel_id is not None:
            update_data["channel_id"] = channel_id
        if metadata is not None:
            update_data["metadata"] = metadata
        
        # updated_atæ›´æ–°
        update_data["updated_at"] = datetime.now(timezone.utc)
        
        try:
            # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆãƒ»ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            updated_task = task.model_copy(update=update_data)
            
            # Redis + PostgreSQLæ›´æ–°
            await self._save_task_to_redis(updated_task)
            await self._save_task_to_database(updated_task)
            
            logger.info(f"Task updated successfully: {task_id}")
            return updated_task
            
        except Exception as e:
            if "validation" in str(e).lower():
                raise TaskValidationError(f"Task validation failed: {e}") from e
            else:
                raise TaskError(f"Task update failed: {e}") from e
    
    async def update_task_metadata(
        self,
        task_id: UUID,
        new_metadata: Dict[str, Any]
    ) -> TaskModel:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°ï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰
        
        Args:
            task_id: ã‚¿ã‚¹ã‚¯ID
            new_metadata: è¿½åŠ ãƒ»æ›´æ–°ã™ã‚‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            TaskModel: æ›´æ–°å¾Œã®ã‚¿ã‚¹ã‚¯
        """
        task = await self.get_task(task_id)
        
        # æ—¢å­˜ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨æ–°ã—ã„ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒžãƒ¼ã‚¸
        merged_metadata = {**task.metadata, **new_metadata}
        
        return await self.update_task(task_id, metadata=merged_metadata)
    
    # DELETE operations
    async def delete_task(self, task_id: UUID) -> None:
        """
        ã‚¿ã‚¹ã‚¯å‰Šé™¤ï¼ˆãƒãƒ¼ãƒ‰å‰Šé™¤ï¼šRedis + PostgreSQL ã‹ã‚‰å®Œå…¨å‰Šé™¤ï¼‰
        
        Args:
            task_id: å‰Šé™¤å¯¾è±¡ã‚¿ã‚¹ã‚¯ID
            
        Raises:
            TaskNotFoundError: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        """
        # ã‚¿ã‚¹ã‚¯å­˜åœ¨ç¢ºèª
        await self.get_task(task_id)
        
        try:
            # Rediså‰Šé™¤
            await self.redis_client.hdel("tasks", str(task_id))
            
            # PostgreSQLå‰Šé™¤
            query = "DELETE FROM tasks WHERE id = $1"
            await self.db_manager.execute(query, task_id)
            
            logger.info(f"Task deleted successfully: {task_id}")
            
        except Exception as e:
            raise TaskError(f"Task deletion failed: {e}") from e
    
    async def soft_delete_task(self, task_id: UUID) -> TaskModel:
        """
        ã‚½ãƒ•ãƒˆå‰Šé™¤ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’CANCELLEDã«å¤‰æ›´ï¼‰
        
        Args:
            task_id: ã‚½ãƒ•ãƒˆå‰Šé™¤å¯¾è±¡ã‚¿ã‚¹ã‚¯ID
            
        Returns:
            TaskModel: ã‚­ãƒ£ãƒ³ã‚»ãƒ«çŠ¶æ…‹ã®ã‚¿ã‚¹ã‚¯
        """
        return await self.update_task(task_id, status=TaskStatus.CANCELLED)
    
    async def bulk_delete_tasks(self, task_ids: List[UUID]) -> int:
        """
        ä¸€æ‹¬å‰Šé™¤
        
        Args:
            task_ids: å‰Šé™¤å¯¾è±¡ã‚¿ã‚¹ã‚¯IDãƒªã‚¹ãƒˆ
            
        Returns:
            int: å‰Šé™¤ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯æ•°
        """
        deleted_count = 0
        for task_id in task_ids:
            try:
                await self.delete_task(task_id)
                deleted_count += 1
            except TaskNotFoundError:
                logger.warning(f"Task not found during bulk delete: {task_id}")
            except Exception as e:
                logger.error(f"Task deletion failed during bulk delete: {task_id}, {e}")
        
        return deleted_count
    
    # Statistics and Health Check
    async def get_statistics(self) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆæƒ…å ±å–å¾—"""
        try:
            stats_query = """
            SELECT 
                COUNT(*) as total_tasks,
                COUNT(CASE WHEN status = 'pending' THEN 1 END) as pending_tasks,
                COUNT(CASE WHEN status = 'in_progress' THEN 1 END) as in_progress_tasks,
                COUNT(CASE WHEN status = 'completed' THEN 1 END) as completed_tasks,
                COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_tasks,
                COUNT(CASE WHEN status = 'cancelled' THEN 1 END) as cancelled_tasks
            FROM tasks
            """
            
            rows = await self.db_manager.fetch(stats_query)
            if rows:
                return dict(rows[0])
            else:
                return {
                    "total_tasks": 0,
                    "pending_tasks": 0,
                    "in_progress_tasks": 0,
                    "completed_tasks": 0,
                    "failed_tasks": 0,
                    "cancelled_tasks": 0
                }
        except Exception as e:
            logger.error(f"Statistics retrieval failed: {e}")
            return {}
    
    async def health_check(self) -> Dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        health_status = {
            "redis": False,
            "database": False,
            "status": "unhealthy"
        }
        
        # RedisæŽ¥ç¶šç¢ºèª
        try:
            await self.redis_client.ping()
            health_status["redis"] = True
        except Exception as e:
            logger.warning(f"Redis health check failed: {e}")
        
        # DatabaseæŽ¥ç¶šç¢ºèª
        try:
            health_status["database"] = await self.db_manager.health_check()
        except Exception as e:
            logger.warning(f"Database health check failed: {e}")
        
        # ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        if health_status["redis"] and health_status["database"]:
            health_status["status"] = "healthy"
        elif health_status["database"]:
            health_status["status"] = "degraded"  # Redisãªã—ã§ã‚‚å‹•ä½œå¯èƒ½
        
        return health_status
    
    # Private helper methods
    async def _save_task_to_redis(self, task: TaskModel) -> None:
        """Redisã«ã‚¿ã‚¹ã‚¯ä¿å­˜"""
        try:
            task_json = task.model_dump_json()
            await self.redis_client.hset("tasks", str(task.id), task_json)
            
            # TTLè¨­å®š
            await self.redis_client.expire(f"task:{task.id}", self.settings.task.default_ttl)
            
        except Exception as e:
            logger.warning(f"Redis task save failed: {e}")
            # Rediså¤±æ•—ã¯ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã‚’åœæ­¢ã•ã›ãªã„
    
    async def _save_task_to_database(self, task: TaskModel) -> None:
        """PostgreSQLã«ã‚¿ã‚¹ã‚¯ä¿å­˜"""
        try:
            query = """
            INSERT INTO tasks (id, title, description, status, priority, agent_id, channel_id, 
                             created_at, updated_at, metadata)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            ON CONFLICT (id) DO UPDATE SET
                title = EXCLUDED.title,
                description = EXCLUDED.description,
                status = EXCLUDED.status,
                priority = EXCLUDED.priority,
                agent_id = EXCLUDED.agent_id,
                channel_id = EXCLUDED.channel_id,
                updated_at = EXCLUDED.updated_at,
                metadata = EXCLUDED.metadata
            """
            
            await self.db_manager.execute(
                query,
                task.id,
                task.title,
                task.description,
                task.status.value,
                task.priority.value,
                task.agent_id,
                task.channel_id,
                task.created_at,
                task.updated_at,
                json.dumps(task.metadata)
            )
            
        except Exception as e:
            logger.error(f"Database task save failed: {e}")
            raise  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å¤±æ•—ã¯é‡è¦ã‚¨ãƒ©ãƒ¼


# RedisTaskQueue Implementation
class RedisTaskQueue:
    """
    Redis ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ç®¡ç†ã‚¯ãƒ©ã‚¹
    
    Phase 4.3: RedisTaskQueueå®Ÿè£…
    - å„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
    - FIFOå‡¦ç†ï¼ˆåŒå„ªå…ˆåº¦å†…ï¼‰
    - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã‚­ãƒ¥ãƒ¼ç®¡ç†
    - ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã‚¿ã‚¹ã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰
    - ãƒ‘ãƒ–ã‚µãƒ–ã‚¤ãƒ™ãƒ³ãƒˆé€šçŸ¥
    - TTLåˆ¶å¾¡ãƒ»æœŸé™åˆ‡ã‚Œã‚¿ã‚¹ã‚¯ç®¡ç†
    """
    
    def __init__(self, settings: Settings):
        """
        RedisTaskQueueåˆæœŸåŒ–
        
        Args:
            settings: è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.settings = settings
        self.redis_url = settings.database.redis_url
        self.max_queue_size = settings.task.max_queue_size
        self.default_ttl = settings.task.default_ttl
        self.max_retry_count = settings.task.max_retry_count
        self.base_retry_delay = settings.task.base_retry_delay
        
        self.redis_client: Optional[redis.Redis] = None
        self.pubsub_client: Optional[redis.Redis] = None
        
        logger.info("RedisTaskQueue initialized")
    
    async def initialize(self) -> None:
        """RedisæŽ¥ç¶šåˆæœŸåŒ–"""
        try:
            self.redis_client = redis.from_url(
                self.redis_url,
                encoding='utf-8',
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5
            )
            
            self.pubsub_client = redis.from_url(
                self.redis_url,
                encoding='utf-8',
                decode_responses=True
            )
            
            logger.info("RedisTaskQueue connections initialized")
            
        except Exception as e:
            error_msg = f"RedisTaskQueue initialization failed: {e}"
            logger.critical(error_msg)
            raise QueueError(error_msg) from e
    
    async def close(self) -> None:
        """RedisæŽ¥ç¶šçµ‚äº†"""
        if self.redis_client:
            await self.redis_client.close()
            self.redis_client = None
        
        if self.pubsub_client:
            await self.pubsub_client.close()
            self.pubsub_client = None
            
        logger.info("RedisTaskQueue connections closed")
    
    # Core Queue Operations
    async def enqueue(self, task: TaskModel, ttl: Optional[int] = None) -> None:
        """
        ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ï¼ˆå„ªå…ˆåº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        
        Args:
            task: ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°å¯¾è±¡ã‚¿ã‚¹ã‚¯
            ttl: TTLï¼ˆç§’ï¼‰
            
        Raises:
            QueueFullError: ã‚­ãƒ¥ãƒ¼ãŒæº€æ¯ã®å ´åˆ
        """
        # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºç¢ºèª
        current_size = await self.get_queue_size()
        if current_size >= self.max_queue_size:
            raise QueueFullError(f"Queue is full (max: {self.max_queue_size})")
        
        try:
            # å„ªå…ˆåº¦åˆ¥ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
            priority_queue_key = f"tasks_queue:{task.priority.value}"
            task_json = task.model_dump_json()
            
            await self.redis_client.lpush(priority_queue_key, task_json)
            
            # TTLè¨­å®š
            ttl_seconds = ttl or self.default_ttl
            await self.redis_client.expire(f"task:{task.id}", ttl_seconds)
            
            # ã‚¤ãƒ™ãƒ³ãƒˆé€šçŸ¥
            await self.publish_event("task_enqueued", {"task_id": str(task.id), "priority": task.priority.value})
            
            logger.info(f"Task enqueued: {task.id} (priority: {task.priority.value})")
            
        except Exception as e:
            raise QueueError(f"Task enqueue failed: {e}") from e
    
    async def dequeue(self, timeout: Optional[float] = None) -> TaskModel:
        """
        ã‚¿ã‚¹ã‚¯ãƒ‡ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ï¼ˆå„ªå…ˆåº¦é †ï¼‰
        
        Args:
            timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰
            
        Returns:
            TaskModel: ãƒ‡ã‚­ãƒ¥ãƒ¼ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯
            
        Raises:
            QueueEmptyError: ã‚­ãƒ¥ãƒ¼ãŒç©ºã®å ´åˆ
        """
        # å„ªå…ˆåº¦é †ï¼ˆCRITICAL -> HIGH -> MEDIUM -> LOWï¼‰
        priority_order = [
            TaskPriority.CRITICAL,
            TaskPriority.HIGH,
            TaskPriority.MEDIUM,
            TaskPriority.LOW
        ]
        
        for priority in priority_order:
            priority_queue_key = f"tasks_queue:{priority.value}"
            
            try:
                if timeout:
                    # ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ãƒ‡ã‚­ãƒ¥ãƒ¼
                    result = await self.redis_client.brpop(priority_queue_key, timeout=timeout)
                    if result:
                        _, task_json = result
                        task_data = json.loads(task_json)
                        task = TaskModel(**task_data)
                        
                        # ã‚¤ãƒ™ãƒ³ãƒˆé€šçŸ¥
                        await self.publish_event("task_dequeued", {"task_id": str(task.id), "priority": priority.value})
                        
                        logger.info(f"Task dequeued: {task.id} (priority: {priority.value})")
                        return task
                else:
                    # ãƒŽãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ãƒ‡ã‚­ãƒ¥ãƒ¼
                    task_json = await self.redis_client.rpop(priority_queue_key)
                    if task_json:
                        task_data = json.loads(task_json)
                        task = TaskModel(**task_data)
                        
                        # ã‚¤ãƒ™ãƒ³ãƒˆé€šçŸ¥
                        await self.publish_event("task_dequeued", {"task_id": str(task.id), "priority": priority.value})
                        
                        logger.info(f"Task dequeued: {task.id} (priority: {priority.value})")
                        return task
                        
            except Exception as e:
                logger.error(f"Dequeue error for priority {priority.value}: {e}")
                continue
        
        raise QueueEmptyError("No tasks available in queue")
    
    # Agent-specific queue operations
    async def enqueue_to_agent(self, task: TaskModel, agent_id: str) -> None:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã‚­ãƒ¥ãƒ¼ã«ã‚¿ã‚¹ã‚¯è¿½åŠ """
        agent_queue_key = f"agent_queue:{agent_id}"
        task_json = task.model_dump_json()
        
        await self.redis_client.lpush(agent_queue_key, task_json)
        logger.info(f"Task enqueued to agent {agent_id}: {task.id}")
    
    async def dequeue_from_agent(self, agent_id: str, timeout: Optional[float] = None) -> TaskModel:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¿ã‚¹ã‚¯å–å¾—"""
        agent_queue_key = f"agent_queue:{agent_id}"
        
        if timeout:
            result = await self.redis_client.brpop(agent_queue_key, timeout=timeout)
            if result:
                _, task_json = result
                task_data = json.loads(task_json)
                return TaskModel(**task_data)
        else:
            task_json = await self.redis_client.rpop(agent_queue_key)
            if task_json:
                task_data = json.loads(task_json)
                return TaskModel(**task_data)
        
        raise QueueEmptyError(f"No tasks available for agent {agent_id}")
    
    async def get_agent_queue_size(self, agent_id: str) -> int:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºå–å¾—"""
        agent_queue_key = f"agent_queue:{agent_id}"
        return await self.redis_client.llen(agent_queue_key)
    
    async def get_active_agents(self) -> List[str]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸€è¦§å–å¾—"""
        pattern = "agent_queue:*"
        keys = await self.redis_client.keys(pattern)
        return [key.split(":")[-1] for key in keys if await self.redis_client.llen(key) > 0]
    
    # Channel-based filtering
    async def get_tasks_by_channel(self, channel_id: str) -> List[TaskModel]:
        """ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥ã‚¿ã‚¹ã‚¯å–å¾—"""
        # å®Ÿè£…ã¯ç°¡ç•¥åŒ–ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚ŠåŠ¹çŽ‡çš„ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒå¿…è¦ï¼‰
        all_tasks = []
        priority_order = [TaskPriority.CRITICAL, TaskPriority.HIGH, TaskPriority.MEDIUM, TaskPriority.LOW]
        
        for priority in priority_order:
            priority_queue_key = f"tasks_queue:{priority.value}"
            tasks_json = await self.redis_client.lrange(priority_queue_key, 0, -1)
            
            for task_json in tasks_json:
                task_data = json.loads(task_json)
                task = TaskModel(**task_data)
                if task.channel_id == channel_id:
                    all_tasks.append(task)
        
        return all_tasks
    
    async def set_channel_limit(self, channel_id: str, limit: int) -> None:
        """ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥åˆ¶é™è¨­å®š"""
        await self.redis_client.hset("channel_limits", channel_id, limit)
    
    # Retry mechanism
    async def mark_task_for_retry(
        self, 
        task_id: UUID, 
        error_message: str,
        backoff_seconds: Optional[int] = None
    ) -> None:
        """ã‚¿ã‚¹ã‚¯ãƒªãƒˆãƒ©ã‚¤ãƒžãƒ¼ã‚¯ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰"""
        retry_key = f"task_retry:{task_id}"
        
        # ç¾åœ¨ã®ãƒªãƒˆãƒ©ã‚¤æƒ…å ±å–å¾—
        retry_info = await self.get_task_retry_info(task_id)
        retry_count = retry_info.get("retry_count", 0) + 1
        
        if retry_count > self.max_retry_count:
            # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›žæ•°è¶…éŽ -> å¤±æ•—ã‚­ãƒ¥ãƒ¼ã¸ç§»å‹•
            await self._move_to_failed_queue(task_id, error_message)
            return
        
        # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—
        if backoff_seconds is None:
            backoff_seconds = self.base_retry_delay * (2 ** (retry_count - 1))
        
        # ãƒªãƒˆãƒ©ã‚¤æƒ…å ±æ›´æ–°
        retry_data = {
            "retry_count": retry_count,
            "error_message": error_message,
            "next_retry_delay": backoff_seconds,
            "next_retry_at": (datetime.now(timezone.utc) + timedelta(seconds=backoff_seconds)).isoformat()
        }
        
        await self.redis_client.hset(retry_key, mapping=retry_data)
        await self.redis_client.expire(retry_key, backoff_seconds + 3600)  # ä½™è£•ã‚’ã‚‚ãŸã›ãŸTTL
        
        logger.info(f"Task marked for retry: {task_id} (count: {retry_count})")
    
    async def get_task_retry_info(self, task_id: UUID) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯ãƒªãƒˆãƒ©ã‚¤æƒ…å ±å–å¾—"""
        retry_key = f"task_retry:{task_id}"
        retry_data = await self.redis_client.hgetall(retry_key)
        
        if retry_data:
            return {
                "retry_count": int(retry_data.get("retry_count", 0)),
                "error_message": retry_data.get("error_message", ""),
                "next_retry_delay": int(retry_data.get("next_retry_delay", 0)),
                "next_retry_at": retry_data.get("next_retry_at", "")
            }
        else:
            return {"retry_count": 0}
    
    async def get_retry_ready_tasks(self) -> List[TaskModel]:
        """ãƒªãƒˆãƒ©ã‚¤æº–å‚™å®Œäº†ã‚¿ã‚¹ã‚¯å–å¾—"""
        # ç°¡ç•¥åŒ–å®Ÿè£…ï¼ˆå®Ÿéš›ã¯ã‚ˆã‚ŠåŠ¹çŽ‡çš„ãªæ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
        retry_pattern = "task_retry:*"
        retry_keys = await self.redis_client.keys(retry_pattern)
        
        ready_tasks = []
        now = datetime.now(timezone.utc)
        
        for retry_key in retry_keys:
            retry_data = await self.redis_client.hgetall(retry_key)
            if retry_data:
                next_retry_at = datetime.fromisoformat(retry_data.get("next_retry_at", ""))
                if now >= next_retry_at:
                    task_id = retry_key.split(":")[-1]
                    # ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦TaskModelã‚’æ§‹ç¯‰ï¼ˆç°¡ç•¥åŒ–ï¼‰
                    # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚ŠåŠ¹çŽ‡çš„ãªæ–¹æ³•ã‚’ä½¿ç”¨
        
        return ready_tasks
    
    async def get_failed_tasks(self) -> List[TaskModel]:
        """å¤±æ•—ã‚¿ã‚¹ã‚¯ä¸€è¦§å–å¾—"""
        failed_queue_key = "failed_tasks"
        tasks_json = await self.redis_client.lrange(failed_queue_key, 0, -1)
        
        failed_tasks = []
        for task_json in tasks_json:
            task_data = json.loads(task_json)
            failed_tasks.append(TaskModel(**task_data))
        
        return failed_tasks
    
    # TTL and expiration management
    async def task_exists(self, task_id: UUID) -> bool:
        """ã‚¿ã‚¹ã‚¯å­˜åœ¨ç¢ºèª"""
        task_key = f"task:{task_id}"
        return bool(await self.redis_client.exists(task_key))
    
    async def get_task_ttl(self, task_id: UUID) -> int:
        """ã‚¿ã‚¹ã‚¯TTLå–å¾—"""
        task_key = f"task:{task_id}"
        return await self.redis_client.ttl(task_key)
    
    async def extend_task_ttl(self, task_id: UUID, additional_seconds: int) -> None:
        """ã‚¿ã‚¹ã‚¯TTLå»¶é•·"""
        task_key = f"task:{task_id}"
        current_ttl = await self.redis_client.ttl(task_key)
        if current_ttl > 0:
            new_ttl = current_ttl + additional_seconds
            await self.redis_client.expire(task_key, new_ttl)
    
    async def cleanup_expired_tasks(self) -> int:
        """æœŸé™åˆ‡ã‚Œã‚¿ã‚¹ã‚¯å‰Šé™¤"""
        # Redis ã®è‡ªå‹• TTL ã«ã‚ˆã‚Šè‡ªå‹•å‰Šé™¤ã•ã‚Œã‚‹
        # ã“ã“ã§ã¯ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®å®Ÿè£…ã¨ã—ã¦ç°¡ç•¥åŒ–
        return 0  # å®Ÿè£…ä¾å­˜
    
    async def get_expiring_tasks(self, threshold_seconds: int) -> List[TaskModel]:
        """æœŸé™é–“è¿‘ã‚¿ã‚¹ã‚¯å–å¾—"""
        # ç°¡ç•¥åŒ–å®Ÿè£…
        return []  # å®Ÿè£…ä¾å­˜
    
    # Pub/Sub event system
    async def publish_event(self, event_type: str, event_data: Dict[str, Any]) -> None:
        """ã‚¤ãƒ™ãƒ³ãƒˆç™ºè¡Œ"""
        try:
            event_payload = {
                "event_type": event_type,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "data": event_data
            }
            
            await self.pubsub_client.publish(f"task_events:{event_type}", json.dumps(event_payload))
            
        except Exception as e:
            logger.warning(f"Event publish failed: {e}")
    
    async def subscribe_to_events(self, event_type: str, handler) -> None:
        """ã‚¤ãƒ™ãƒ³ãƒˆè³¼èª­ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è³¼èª­å‡¦ç†ã‚’å®Ÿè£…
        pass
    
    async def subscribe_to_task_events(self, handler) -> None:
        """ã‚¿ã‚¹ã‚¯ã‚¤ãƒ™ãƒ³ãƒˆè³¼èª­ï¼ˆç°¡ç•¥åŒ–å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯åŒ…æ‹¬çš„ãªã‚¤ãƒ™ãƒ³ãƒˆå‡¦ç†ã‚’å®Ÿè£…
        pass
    
    # Queue statistics and utilities
    async def get_queue_size(self) -> int:
        """ç·ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºå–å¾—"""
        total_size = 0
        for priority in TaskPriority:
            priority_queue_key = f"tasks_queue:{priority.value}"
            size = await self.redis_client.llen(priority_queue_key)
            total_size += size
        return total_size
    
    async def get_queue_size_by_priority(self, priority: TaskPriority) -> int:
        """å„ªå…ˆåº¦åˆ¥ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºå–å¾—"""
        priority_queue_key = f"tasks_queue:{priority.value}"
        return await self.redis_client.llen(priority_queue_key)
    
    async def get_statistics(self) -> Dict[str, Any]:
        """ã‚­ãƒ¥ãƒ¼çµ±è¨ˆæƒ…å ±å–å¾—"""
        stats = {
            "total_tasks": await self.get_queue_size(),
            "tasks_by_priority": {},
            "queue_size": await self.get_queue_size(),
            "active_agents": len(await self.get_active_agents())
        }
        
        for priority in TaskPriority:
            size = await self.get_queue_size_by_priority(priority)
            stats["tasks_by_priority"][priority.value] = size
        
        return stats
    
    async def health_check(self) -> Dict[str, Any]:
        """ã‚­ãƒ¥ãƒ¼ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        health_status = {
            "redis_connection": False,
            "queue_size": 0,
            "status": "unhealthy"
        }
        
        try:
            await self.redis_client.ping()
            health_status["redis_connection"] = True
            health_status["queue_size"] = await self.get_queue_size()
            health_status["status"] = "healthy"
        except Exception as e:
            logger.error(f"Queue health check failed: {e}")
        
        return health_status
    
    # Private helper methods
    async def _move_to_failed_queue(self, task_id: UUID, error_message: str) -> None:
        """å¤±æ•—ã‚­ãƒ¥ãƒ¼ã¸ã‚¿ã‚¹ã‚¯ç§»å‹•"""
        failed_queue_key = "failed_tasks"
        
        # ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’å¤±æ•—ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆè©³ç´°ã¯å®Ÿè£…ä¾å­˜ï¼‰
        failed_info = {
            "task_id": str(task_id),
            "error_message": error_message,
            "failed_at": datetime.now(timezone.utc).isoformat()
        }
        
        await self.redis_client.lpush(failed_queue_key, json.dumps(failed_info))
        logger.error(f"Task moved to failed queue: {task_id}")


# Singleton pattern helpers
_task_manager_instance: Optional[TaskManager] = None
_redis_queue_instance: Optional[RedisTaskQueue] = None


def get_task_manager() -> TaskManager:
    """TaskManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _task_manager_instance
    if _task_manager_instance is None:
        settings = get_settings()
        _task_manager_instance = TaskManager(settings)
    return _task_manager_instance


def reset_task_manager() -> None:
    """TaskManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒªã‚»ãƒƒãƒˆï¼ˆä¸»ã«ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    global _task_manager_instance
    _task_manager_instance = None


def get_redis_queue() -> RedisTaskQueue:
    """RedisTaskQueueã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""
    global _redis_queue_instance
    if _redis_queue_instance is None:
        settings = get_settings()
        _redis_queue_instance = RedisTaskQueue(settings)
    return _redis_queue_instance


def reset_redis_queue() -> None:
    """RedisTaskQueueã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒªã‚»ãƒƒãƒˆï¼ˆä¸»ã«ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    global _redis_queue_instance
    _redis_queue_instance = None


# Convenience helper functions
async def initialize_task_system() -> tuple[TaskManager, RedisTaskQueue]:
    """ã‚¿ã‚¹ã‚¯ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    task_manager = get_task_manager()
    redis_queue = get_redis_queue()
    
    await task_manager.initialize()
    await redis_queue.initialize()
    
    logger.info("Task system initialized successfully")
    return task_manager, redis_queue


async def close_task_system() -> None:
    """ã‚¿ã‚¹ã‚¯ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
    task_manager = get_task_manager()
    redis_queue = get_redis_queue()
    
    await task_manager.close()
    await redis_queue.close()
    
    logger.info("Task system closed successfully")