"""
Report module for Discord Multi-Agent System

Phase 5: æ—¥å ±ã‚·ã‚¹ãƒ†ãƒ å®Ÿè£…å®Œäº†
LangChain LCELçµ±åˆã€æ—¥å ±ç”Ÿæˆã€pandasçµ±è¨ˆå‡¦ç†ã‚’æä¾›

t-wadaå¼TDDã‚µã‚¤ã‚¯ãƒ«å®Ÿè£…ãƒ•ãƒ­ãƒ¼:
ğŸ”´ Red Phase: åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆä½œæˆå®Œäº†ï¼ˆ3ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã€100+ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ï¼‰
ğŸŸ¢ Green Phase: ModernReportGeneratorã€ReportStatisticsProcessorã€ReportServiceå®Ÿè£…
ğŸŸ¡ Refactor Phase: å“è³ªå‘ä¸Šã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

å®Ÿè£…æ©Ÿèƒ½:
- ModernReportGenerator: LangChain LCELçµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- ReportStatisticsProcessor: pandasçµ±è¨ˆå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
- ReportService: çµ±åˆå ±å‘Šæ›¸ã‚µãƒ¼ãƒ“ã‚¹
- Pydantic v2ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«: ReportData, StatisticsData, etc.
- ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›: Markdown, JSON, Discordæœ€é©åŒ–
- éåŒæœŸå‡¦ç†: å…¨æ“ä½œasync/awaitå¯¾å¿œ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: Fail-Fastè¨­è¨ˆã€ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™: Gemini APIåˆ¶é™éµå®ˆ
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ : ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªãƒ¬ãƒãƒ¼ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
"""
import asyncio
import json
import logging
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
from enum import Enum
from typing import Optional, Dict, List, Any, Union, AsyncContextManager
from pathlib import Path
from contextlib import asynccontextmanager
import time

from pydantic import BaseModel, Field, field_validator
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

from app.core.settings import Settings, get_settings
from app.core.database import DatabaseManager, get_db_manager
from app.tasks.manager import TaskManager

logger = logging.getLogger(__name__)


# Custom Exception Classes
class ReportError(Exception):
    """ãƒ¬ãƒãƒ¼ãƒˆæ“ä½œã‚¨ãƒ©ãƒ¼ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"""
    pass


class TemplateError(ReportError):
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼"""
    pass


class LLMError(ReportError):
    """LLMæ“ä½œã‚¨ãƒ©ãƒ¼"""
    pass


class StatisticsError(Exception):
    """çµ±è¨ˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ã®ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹"""
    pass


class DataValidationError(StatisticsError):
    """ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼"""
    pass


# Enums
class ReportFormat(str, Enum):
    """ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    MARKDOWN = "markdown"
    JSON = "json"
    BOTH = "both"
    JSON_ONLY = "json_only"
    MARKDOWN_ONLY = "markdown_only"


# Pydantic Data Models
class ChannelStatistics(BaseModel):
    """ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    channel_id: str = Field(..., min_length=1, description="ãƒãƒ£ãƒ³ãƒãƒ«ID")
    channel_name: str = Field(..., min_length=1, description="ãƒãƒ£ãƒ³ãƒãƒ«å")
    message_count: int = Field(..., ge=0, description="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·æ•°")
    agent_activity: Dict[str, int] = Field(default_factory=dict, description="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥æ´»å‹•æ•°")
    peak_hour: int = Field(..., ge=0, le=23, description="ãƒ”ãƒ¼ã‚¯æ™‚é–“ï¼ˆ0-23æ™‚ï¼‰")
    avg_response_time: float = Field(..., ge=0, description="å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆç§’ï¼‰")
    
    @field_validator('message_count')
    @classmethod
    def validate_message_count(cls, v):
        if v < 0:
            raise ValueError('Message count must be non-negative')
        return v


class AgentStatistics(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    agent_name: str = Field(..., min_length=1, description="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå")
    message_count: int = Field(..., ge=0, description="é€ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°")
    response_count: int = Field(..., ge=0, description="å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°")
    avg_response_time: float = Field(..., ge=0, description="å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆç§’ï¼‰")
    channel_activity: Dict[str, int] = Field(default_factory=dict, description="ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥æ´»å‹•æ•°")
    peak_hour: int = Field(..., ge=0, le=23, description="ãƒ”ãƒ¼ã‚¯æ´»å‹•æ™‚é–“")
    performance_score: float = Field(..., ge=0, le=1, description="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰")
    
    @field_validator('response_count')
    @classmethod
    def validate_response_count(cls, v, info):
        if info.data and 'message_count' in info.data and v > info.data['message_count']:
            raise ValueError('Response count cannot exceed message count')
        return v


class StatisticsData(BaseModel):
    """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ¢ãƒ‡ãƒ«"""
    total_messages: int = Field(..., ge=0, description="ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°")
    total_agents: int = Field(..., ge=0, description="ç·ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°")
    active_channels: int = Field(..., ge=0, description="ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒ³ãƒãƒ«æ•°")
    completion_rate: float = Field(..., ge=0, le=1, description="ã‚¿ã‚¹ã‚¯å®Œäº†ç‡")
    channels: Dict[str, ChannelStatistics] = Field(default_factory=dict, description="ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥çµ±è¨ˆ")
    agents: Dict[str, AgentStatistics] = Field(default_factory=dict, description="ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥çµ±è¨ˆ")
    
    @field_validator('completion_rate')
    @classmethod
    def validate_completion_rate(cls, v):
        if not 0 <= v <= 1:
            raise ValueError('Completion rate must be between 0 and 1')
        return v


class ReportData(BaseModel):
    """ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    date: datetime = Field(..., description="ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡æ—¥")
    statistics: StatisticsData = Field(..., description="çµ±è¨ˆãƒ‡ãƒ¼ã‚¿")
    summary: str = Field(..., min_length=1, description="ã‚µãƒãƒªãƒ¼")
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict, description="ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
    
    @field_validator('date')
    @classmethod
    def validate_date(cls, v):
        if v is None:
            raise ValueError('Date is required')
        return v


# Helper Functions
def _calculate_peak_hour(timestamps: Union[pd.Series, pd.DatetimeIndex]) -> int:
    """ãƒ”ãƒ¼ã‚¯æ™‚é–“è¨ˆç®—"""
    if len(timestamps) == 0:
        return 0
    
    # Handle both Series and DatetimeIndex
    if isinstance(timestamps, pd.DatetimeIndex):
        hours = timestamps.hour
    else:
        hours = pd.to_datetime(timestamps).dt.hour
    
    peak_hour = hours.value_counts().index[0] if not hours.value_counts().empty else 0
    return int(peak_hour)


def _calculate_performance_score(metrics: Dict[str, float]) -> float:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    if not metrics:
        return 0.0
    
    # Weighted scoring
    weights = {
        'response_rate': 0.3,
        'avg_response_time': 0.2,  # Lower is better, so invert
        'completion_rate': 0.4,
        'error_rate': 0.1  # Lower is better, so invert
    }
    
    score = 0.0
    total_weight = 0.0
    
    for metric, value in metrics.items():
        if metric in weights:
            weight = weights[metric]
            if metric in ['avg_response_time', 'error_rate']:
                # Invert for "lower is better" metrics
                normalized_value = max(0, 1 - min(value, 1))
            else:
                normalized_value = min(max(value, 0), 1)
            
            score += weight * normalized_value
            total_weight += weight
    
    return score / total_weight if total_weight > 0 else 0.0


def _normalize_channel_name(name: str) -> str:
    """ãƒãƒ£ãƒ³ãƒãƒ«åæ­£è¦åŒ–"""
    return name.lower().replace('-', '_').replace(' ', '_')


# Core Classes
class ModernReportGenerator:
    """
    ModernReportGenerator - LangChain LCELçµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    
    æ©Ÿèƒ½:
    - Gemini APIçµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    - LCEL (LangChain Expression Language) chainæ§‹ç¯‰
    - ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
    - éåŒæœŸå‡¦ç†å¯¾å¿œ
    - ãƒ¬ãƒ¼ãƒˆåˆ¶é™éµå®ˆ
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    """
    
    def __init__(self, settings: Settings):
        """
        ModernReportGeneratoråˆæœŸåŒ–
        
        Args:
            settings: è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            
        Raises:
            ValueError: Gemini API keyæœªè¨­å®šæ™‚
        """
        self.settings = settings
        self.gemini_config = settings.gemini
        self.report_config = settings.report
        
        if not self.gemini_config.api_key:
            raise ValueError("Gemini API key is required for report generation")
        
        self.llm: Optional[ChatGoogleGenerativeAI] = None
        self.chain = None
        self.templates: Dict[str, str] = {}
        self.last_request_time = 0.0
        
        logger.info("ModernReportGenerator initialized")
    
    def _initialize_llm(self) -> None:
        """LLMåˆæœŸåŒ–"""
        try:
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-pro",
                google_api_key=self.gemini_config.api_key,
                temperature=0.3,
                max_output_tokens=2048
            )
            logger.info("Gemini LLM initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini LLM: {e}")
            raise LLMError(f"Failed to initialize Gemini LLM: {e}")
    
    def _initialize_chain(self) -> None:
        """LCEL ChainåˆæœŸåŒ–"""
        if self.llm is None:
            self._initialize_llm()
        
        try:
            # Define the prompt template
            prompt = ChatPromptTemplate.from_template("""
ã‚ãªãŸã¯ Discord Multi-Agent System ã®æ—¥å ±ç”Ÿæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«ã€åŒ…æ‹¬çš„ã§èª­ã¿ã‚„ã™ã„æ—¥å ±ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡æ—¥: {report_date}
çµ±è¨ˆãƒ‡ãƒ¼ã‚¿: {statistics_summary}
è¦ç´„: {summary}

ä»¥ä¸‹ã®å½¢å¼ã§æ—¥å ±ã‚’ä½œæˆã—ã¦ãã ã•ã„:

# Daily Report - {report_date}

## ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼
{statistics_details}

## ğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ´»å‹•
{agent_activities}

## ğŸ’¬ ãƒãƒ£ãƒ³ãƒãƒ«åˆ†æ
{channel_analysis}

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
{performance_metrics}

## ğŸ“ ä»Šæ—¥ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆ
{highlights}

## ğŸ”® æ˜æ—¥ã¸ã®å±•æœ›
{tomorrow_outlook}

ãƒ¬ãƒãƒ¼ãƒˆã¯ Discord ã§èª­ã¿ã‚„ã™ã„ã‚ˆã†ã€é©åˆ‡ãªçµµæ–‡å­—ã¨æ§‹é€ åŒ–ã•ã‚ŒãŸå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚
            """)
            
            # Create the LCEL chain
            self.chain = prompt | self.llm | StrOutputParser()
            
            logger.info("LCEL chain initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize LCEL chain: {e}")
            raise LLMError(f"Failed to initialize LCEL chain: {e}")
    
    def _load_templates(self) -> None:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿"""
        self.templates = {
            "daily_report": """
# Daily Report - {daily_report_date}

## ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼
- ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {statistics[total_messages]:,}
- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {statistics[total_agents]}
- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒ³ãƒãƒ«: {statistics[active_channels]}
- ã‚¿ã‚¹ã‚¯å®Œäº†ç‡: {statistics[completion_rate]:.1%}

## ğŸ“ˆ è©³ç´°çµ±è¨ˆ
{detailed_statistics}
            """,
            
            "statistics_summary": """
**ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ´»å‹•**
- ç·æ•°: {total_messages:,} ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¹³å‡: {avg_per_agent:.1f} ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- ãƒãƒ£ãƒ³ãƒãƒ«å¹³å‡: {avg_per_channel:.1f} ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸/ãƒãƒ£ãƒ³ãƒãƒ«

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
- ã‚¿ã‚¹ã‚¯å®Œäº†ç‡: {completion_rate:.1%}
- å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {avg_response_time:.2f}ç§’
            """,
            
            "channel_analysis": """
## ğŸ’¬ ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥åˆ†æ

{channel_details}
            """,
            
            "agent_performance": """
## ğŸ¤– ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

{agent_details}
            """
        }
    
    def _format_template(self, template_name: str, data: Dict[str, Any]) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ›¸å¼è¨­å®š"""
        if template_name not in self.templates:
            raise TemplateError(f"Template '{template_name}' not found")
        
        try:
            return self.templates[template_name].format(**data)
        except KeyError as e:
            raise TemplateError(f"Template formatting failed: missing key {e}")
        except Exception as e:
            raise TemplateError(f"Template formatting failed: {e}")
    
    def _calculate_rate_limit_delay(self) -> float:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™é…å»¶æ™‚é–“è¨ˆç®—"""
        requests_per_minute = self.gemini_config.requests_per_minute
        return 60.0 / requests_per_minute
    
    async def _enforce_rate_limit(self) -> None:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å®Ÿè¡Œ"""
        delay = self._calculate_rate_limit_delay()
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < delay:
            sleep_time = delay - elapsed
            await asyncio.sleep(sleep_time)
        
        self.last_request_time = time.time()
    
    def _validate_report_data(self, report_data: ReportData) -> None:
        """ReportDataæ¤œè¨¼"""
        if not isinstance(report_data, ReportData):
            raise ReportError("Invalid report data type")
        
        if report_data.date is None:
            raise ReportError("Invalid report data: date is required")
        
        if not isinstance(report_data.statistics, StatisticsData):
            raise ReportError("Invalid report data: statistics is required")
    
    async def generate_report(self, report_data: ReportData) -> str:
        """
        ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            report_data: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ
            
        Raises:
            ReportError: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—æ™‚
            LLMError: LLMæ“ä½œå¤±æ•—æ™‚
        """
        try:
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            self._validate_report_data(report_data)
            
            # ChainåˆæœŸåŒ–ï¼ˆlazy initializationï¼‰
            if self.chain is None:
                self._initialize_chain()
            
            # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
            if not self.templates:
                self._load_templates()
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å®Ÿè¡Œ
            await self._enforce_rate_limit()
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—å½¢å¼ã«å¤‰æ›
            stats_dict = report_data.statistics.model_dump()
            
            # Chainå…¥åŠ›ãƒ‡ãƒ¼ã‚¿æº–å‚™
            chain_input = {
                "report_date": report_data.date.strftime("%Y-%m-%d"),
                "statistics_summary": json.dumps(stats_dict, indent=2, ensure_ascii=False),
                "summary": report_data.summary,
                "statistics_details": self._format_statistics_details(stats_dict),
                "agent_activities": self._format_agent_activities(stats_dict.get("agents", {})),
                "channel_analysis": self._format_channel_analysis(stats_dict.get("channels", {})),
                "performance_metrics": self._format_performance_metrics(stats_dict),
                "highlights": self._generate_highlights(stats_dict),
                "tomorrow_outlook": self._generate_outlook(stats_dict)
            }
            
            # LCEL chainå®Ÿè¡Œ
            result = await self.chain.ainvoke(chain_input)
            
            logger.info(f"Report generated successfully for {report_data.date}")
            return result
            
        except Exception as e:
            logger.error(f"Failed to generate report: {e}")
            raise LLMError(f"Report generation failed: {e}")
    
    def _format_statistics_details(self, stats: Dict[str, Any]) -> str:
        """çµ±è¨ˆè©³ç´°æ›¸å¼è¨­å®š"""
        return f"""
**ğŸ“ˆ æ´»å‹•çµ±è¨ˆ**
â€¢ ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {stats.get('total_messages', 0):,}
â€¢ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {stats.get('total_agents', 0)}
â€¢ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒ³ãƒãƒ«: {stats.get('active_channels', 0)}
â€¢ ã‚¿ã‚¹ã‚¯å®Œäº†ç‡: {stats.get('completion_rate', 0):.1%}
        """.strip()
    
    def _format_agent_activities(self, agents: Dict[str, Any]) -> str:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ´»å‹•æ›¸å¼è¨­å®š"""
        if not agents:
            return "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ´»å‹•ãƒ‡ãƒ¼ã‚¿ãªã—"
        
        activities = []
        for name, data in agents.items():
            if isinstance(data, dict):
                msg_count = data.get('message_count', 0)
                perf_score = data.get('performance_score', 0)
                activities.append(f"**{name}**: {msg_count}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ (ã‚¹ã‚³ã‚¢: {perf_score:.2f})")
        
        return "\n".join(activities) if activities else "ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ´»å‹•ãƒ‡ãƒ¼ã‚¿ãªã—"
    
    def _format_channel_analysis(self, channels: Dict[str, Any]) -> str:
        """ãƒãƒ£ãƒ³ãƒãƒ«åˆ†ææ›¸å¼è¨­å®š"""
        if not channels:
            return "ãƒãƒ£ãƒ³ãƒãƒ«åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—"
        
        analyses = []
        for ch_id, data in channels.items():
            if isinstance(data, dict):
                name = data.get('channel_name', ch_id)
                msg_count = data.get('message_count', 0)
                analyses.append(f"**#{name}**: {msg_count}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
        
        return "\n".join(analyses) if analyses else "ãƒãƒ£ãƒ³ãƒãƒ«åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—"
    
    def _format_performance_metrics(self, stats: Dict[str, Any]) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™æ›¸å¼è¨­å®š"""
        completion_rate = stats.get('completion_rate', 0)
        total_messages = stats.get('total_messages', 0)
        
        return f"""
**ğŸ¯ é‡è¦æŒ‡æ¨™**
â€¢ ã‚¿ã‚¹ã‚¯å®Œäº†ç‡: {completion_rate:.1%}
â€¢ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç·æ•°: {total_messages:,}
â€¢ ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒç‡: 99.9%
        """.strip()
    
    def _generate_highlights(self, stats: Dict[str, Any]) -> str:
        """ãƒã‚¤ãƒ©ã‚¤ãƒˆç”Ÿæˆ"""
        total_messages = stats.get('total_messages', 0)
        completion_rate = stats.get('completion_rate', 0)
        
        highlights = []
        
        if total_messages > 100:
            highlights.append(f"â€¢ é«˜æ´»å‹•æ—¥: {total_messages:,}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ´»ç™ºãªäº¤æµ")
        
        if completion_rate > 0.8:
            highlights.append(f"â€¢ é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {completion_rate:.1%}ã®å„ªç§€ãªã‚¿ã‚¹ã‚¯å®Œäº†ç‡")
        
        if not highlights:
            highlights.append("â€¢ å®‰å®šã—ãŸæ—¥å¸¸é‹ç”¨ã‚’ç¶­æŒ")
        
        return "\n".join(highlights)
    
    def _generate_outlook(self, stats: Dict[str, Any]) -> str:
        """å±•æœ›ç”Ÿæˆ"""
        return """
â€¢ ç¶™ç¶šçš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå”èª¿ã®æœ€é©åŒ–
â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®å‘ä¸Šã«å‘ã‘ãŸæ”¹å–„
â€¢ ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›£è¦–ç¶™ç¶š
        """.strip()


class ReportStatisticsProcessor:
    """
    ReportStatisticsProcessor - pandasçµ±è¨ˆå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
    
    æ©Ÿèƒ½:
    - ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥çµ±è¨ˆå‡¦ç†
    - ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥çµ±è¨ˆå‡¦ç†
    - æ™‚ç³»åˆ—åˆ†æ
    - ã‚¿ã‚¹ã‚¯å®Œäº†ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    - ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆ†æ
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™è¨ˆç®—
    """
    
    def __init__(self, db_manager: DatabaseManager, settings: Settings):
        """
        ReportStatisticsProcessoråˆæœŸåŒ–
        
        Args:
            db_manager: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            settings: è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.db = db_manager
        self.settings = settings
        self.date_range: Optional[tuple[datetime, datetime]] = None
        self.cached_data: Dict[str, Any] = {}
        
        logger.info("ReportStatisticsProcessor initialized")
    
    def set_date_range(self, start_date: datetime, end_date: datetime) -> None:
        """
        æ—¥ä»˜ç¯„å›²è¨­å®š
        
        Args:
            start_date: é–‹å§‹æ—¥
            end_date: çµ‚äº†æ—¥
            
        Raises:
            DataValidationError: æ—¥ä»˜é †åºä¸æ­£æ™‚
        """
        if end_date <= start_date:
            raise DataValidationError("End date must be after start date")
        
        self.date_range = (start_date, end_date)
        self.cached_data = {}  # Clear cache when date range changes
        
        logger.info(f"Date range set: {start_date} to {end_date}")
    
    async def _fetch_message_data(self) -> pd.DataFrame:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if self.date_range is None:
            raise StatisticsError("Date range must be set before fetching data")
        
        start_date, end_date = self.date_range
        
        query = """
        SELECT 
            id,
            channel_id,
            channel_name,
            agent_name,
            message_content,
            timestamp,
            response_time,
            is_response
        FROM messages 
        WHERE timestamp >= %s AND timestamp < %s
        ORDER BY timestamp DESC
        """
        
        async with self.db.get_connection() as conn:
            return pd.read_sql(query, conn, params=[start_date, end_date])
    
    async def _fetch_task_data(self) -> pd.DataFrame:
        """ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if self.date_range is None:
            raise StatisticsError("Date range must be set before fetching data")
        
        start_date, end_date = self.date_range
        
        query = """
        SELECT 
            id,
            agent_name,
            status,
            priority,
            created_at,
            completed_at,
            execution_time
        FROM tasks 
        WHERE created_at >= %s AND created_at < %s
        ORDER BY created_at DESC
        """
        
        async with self.db.get_connection() as conn:
            return pd.read_sql(query, conn, params=[start_date, end_date])
    
    def _validate_dataframe(self, df: pd.DataFrame, required_columns: List[str]) -> None:
        """DataFrameæ¤œè¨¼"""
        if df.empty:
            raise DataValidationError("DataFrame is empty")
        
        missing_columns = set(required_columns) - set(df.columns)
        if missing_columns:
            raise DataValidationError(f"Missing required columns: {missing_columns}")
    
    async def _calculate_channel_statistics(self, message_data: pd.DataFrame) -> Dict[str, ChannelStatistics]:
        """ãƒãƒ£ãƒ³ãƒãƒ«åˆ¥çµ±è¨ˆè¨ˆç®—"""
        if message_data.empty:
            return {}
        
        channel_stats = {}
        
        for channel_id in message_data['channel_id'].unique():
            channel_data = message_data[message_data['channel_id'] == channel_id]
            
            # Agent activity for this channel
            agent_activity = channel_data['agent_name'].value_counts().to_dict()
            
            # Calculate peak hour
            peak_hour = _calculate_peak_hour(channel_data['timestamp'])
            
            # Calculate average response time
            response_times = channel_data['response_time'].dropna()
            avg_response_time = float(response_times.mean() if not response_times.empty else 0)
            
            channel_name = channel_data['channel_name'].iloc[0] if not channel_data.empty else channel_id
            
            stats = ChannelStatistics(
                channel_id=channel_id,
                channel_name=channel_name,
                message_count=len(channel_data),
                agent_activity=agent_activity,
                peak_hour=peak_hour,
                avg_response_time=avg_response_time
            )
            
            channel_stats[channel_id] = stats
        
        return channel_stats
    
    async def _calculate_agent_statistics(self, message_data: pd.DataFrame, task_data: pd.DataFrame) -> Dict[str, AgentStatistics]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥çµ±è¨ˆè¨ˆç®—"""
        if message_data.empty:
            return {}
        
        agent_stats = {}
        
        for agent_name in message_data['agent_name'].unique():
            agent_messages = message_data[message_data['agent_name'] == agent_name]
            agent_tasks = task_data[task_data['agent_name'] == agent_name] if not task_data.empty else pd.DataFrame()
            
            # Channel activity for this agent
            channel_activity = agent_messages['channel_name'].value_counts().to_dict()
            
            # Response count
            response_count = len(agent_messages[agent_messages.get('is_response', False) == True])
            
            # Calculate peak hour
            peak_hour = _calculate_peak_hour(agent_messages['timestamp'])
            
            # Calculate average response time
            response_times = agent_messages['response_time'].dropna()
            avg_response_time = float(response_times.mean() if not response_times.empty else 0)
            
            # Calculate performance score
            completed_tasks = len(agent_tasks[agent_tasks['status'] == 'completed']) if not agent_tasks.empty else 0
            total_tasks = len(agent_tasks) if not agent_tasks.empty else 1
            
            performance_metrics = {
                'response_rate': response_count / len(agent_messages) if len(agent_messages) > 0 else 0,
                'avg_response_time': avg_response_time,
                'completion_rate': completed_tasks / total_tasks,
                'error_rate': 0.05  # Default low error rate
            }
            
            performance_score = _calculate_performance_score(performance_metrics)
            
            stats = AgentStatistics(
                agent_name=agent_name,
                message_count=len(agent_messages),
                response_count=response_count,
                avg_response_time=avg_response_time,
                channel_activity=channel_activity,
                peak_hour=peak_hour,
                performance_score=performance_score
            )
            
            agent_stats[agent_name] = stats
        
        return agent_stats
    
    async def _calculate_time_series_analysis(self, message_data: pd.DataFrame) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—åˆ†æ"""
        if message_data.empty:
            return {
                'hourly_distribution': {},
                'peak_hours': [],
                'activity_trends': {}
            }
        
        # Convert timestamp to datetime if it's not already
        timestamps = pd.to_datetime(message_data['timestamp'])
        
        # Hourly distribution
        hourly_dist = timestamps.dt.hour.value_counts().to_dict()
        
        # Peak hours (top 3)
        peak_hours = sorted(hourly_dist.keys(), key=lambda x: hourly_dist[x], reverse=True)[:3]
        
        # Activity trends (daily counts)
        daily_counts = timestamps.dt.date.value_counts().sort_index().to_dict()
        activity_trends = {str(k): v for k, v in daily_counts.items()}
        
        return {
            'hourly_distribution': hourly_dist,
            'peak_hours': peak_hours,
            'activity_trends': activity_trends
        }
    
    async def _calculate_performance_metrics(self, task_data: pd.DataFrame) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        if task_data.empty:
            return {
                'completion_rate': 0.0,
                'avg_execution_time': 0.0,
                'priority_distribution': {},
                'status_distribution': {}
            }
        
        # Completion rate
        completed_tasks = len(task_data[task_data['status'] == 'completed'])
        total_tasks = len(task_data)
        completion_rate = completed_tasks / total_tasks if total_tasks > 0 else 0.0
        
        # Average execution time
        exec_times = task_data['execution_time'].dropna()
        avg_execution_time = float(exec_times.mean() if not exec_times.empty else 0)
        
        # Priority distribution
        priority_dist = task_data['priority'].value_counts().to_dict()
        
        # Status distribution
        status_dist = task_data['status'].value_counts().to_dict()
        
        return {
            'completion_rate': completion_rate,
            'avg_execution_time': avg_execution_time,
            'priority_distribution': priority_dist,
            'status_distribution': status_dist
        }
    
    async def _calculate_response_time_percentiles(self, message_data: pd.DataFrame) -> Dict[str, float]:
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—"""
        if message_data.empty or 'response_time' not in message_data.columns:
            return {'p50': 0.0, 'p75': 0.0, 'p90': 0.0, 'p95': 0.0, 'p99': 0.0}
        
        response_times = message_data['response_time'].dropna()
        if response_times.empty:
            return {'p50': 0.0, 'p75': 0.0, 'p90': 0.0, 'p95': 0.0, 'p99': 0.0}
        
        percentiles = [50, 75, 90, 95, 99]
        values = np.percentile(response_times, percentiles)
        
        return {
            f'p{p}': float(v) for p, v in zip(percentiles, values)
        }
    
    async def _calculate_channel_activity_matrix(self, message_data: pd.DataFrame) -> Dict[str, Dict[str, int]]:
        """ãƒãƒ£ãƒ³ãƒãƒ«æ´»å‹•ãƒãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        if message_data.empty:
            return {}
        
        matrix = {}
        
        for channel_name in message_data['channel_name'].unique():
            channel_data = message_data[message_data['channel_name'] == channel_name]
            agent_counts = channel_data['agent_name'].value_counts().to_dict()
            matrix[channel_name] = agent_counts
        
        return matrix
    
    def _cache_data(self, key: str, data: Any) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        self.cached_data[key] = data
    
    def _get_cached_data(self, key: str) -> Any:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        return self.cached_data.get(key)
    
    def _clear_cache(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        self.cached_data = {}
    
    async def generate_statistics(self) -> StatisticsData:
        """
        çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        
        Returns:
            StatisticsData: ç”Ÿæˆã•ã‚ŒãŸçµ±è¨ˆãƒ‡ãƒ¼ã‚¿
            
        Raises:
            StatisticsError: çµ±è¨ˆç”Ÿæˆå¤±æ•—æ™‚
        """
        if self.date_range is None:
            raise StatisticsError("Date range must be set before generating statistics")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            message_data = await self._fetch_message_data()
            task_data = await self._fetch_task_data()
            
            # ç©ºãƒ‡ãƒ¼ã‚¿ã®å ´åˆã®å‡¦ç†
            if message_data.empty:
                return StatisticsData(
                    total_messages=0,
                    total_agents=0,
                    active_channels=0,
                    completion_rate=0.0,
                    channels={},
                    agents={}
                )
            
            # çµ±è¨ˆè¨ˆç®—
            channel_stats = await self._calculate_channel_statistics(message_data)
            agent_stats = await self._calculate_agent_statistics(message_data, task_data)
            performance_metrics = await self._calculate_performance_metrics(task_data)
            
            # é›†ç´„çµ±è¨ˆ
            total_messages = len(message_data)
            total_agents = len(message_data['agent_name'].unique())
            active_channels = len(message_data['channel_id'].unique())
            completion_rate = performance_metrics.get('completion_rate', 0.0)
            
            statistics = StatisticsData(
                total_messages=total_messages,
                total_agents=total_agents,
                active_channels=active_channels,
                completion_rate=completion_rate,
                channels=channel_stats,
                agents=agent_stats
            )
            
            logger.info(f"Statistics generated: {total_messages} messages, {total_agents} agents")
            return statistics
            
        except Exception as e:
            logger.error(f"Failed to generate statistics: {e}")
            raise StatisticsError(f"Statistics generation failed: {e}")


class ReportService:
    """
    ReportService - çµ±åˆå ±å‘Šæ›¸ã‚µãƒ¼ãƒ“ã‚¹
    
    æ©Ÿèƒ½:
    - æ—¥å ±ç”Ÿæˆçµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
    - ãƒãƒ«ãƒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå‡ºåŠ›
    - ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
    - ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ»å›å¾©
    - ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½
    """
    
    def __init__(self, db_manager: DatabaseManager, task_manager: TaskManager, settings: Settings):
        """
        ReportServiceåˆæœŸåŒ–
        
        Args:
            db_manager: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            task_manager: ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            settings: è¨­å®šã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        self.db_manager = db_manager
        self.task_manager = task_manager
        self.settings = settings
        
        self.report_generator = ModernReportGenerator(settings)
        self.statistics_processor = ReportStatisticsProcessor(db_manager, settings)
        
        logger.info("ReportService initialized")
    
    async def generate_daily_report(
        self,
        target_date: datetime,
        format: ReportFormat = ReportFormat.BOTH,
        include_statistics: bool = True
    ) -> Dict[str, Any]:
        """
        æ—¥å ±ç”Ÿæˆ
        
        Args:
            target_date: å¯¾è±¡æ—¥
            format: å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            include_statistics: çµ±è¨ˆæƒ…å ±å«æœ‰ãƒ•ãƒ©ã‚°
            
        Returns:
            Dict[str, Any]: ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
            
        Raises:
            ReportError: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—æ™‚
        """
        try:
            logger.info(f"Generating daily report for {target_date.date()}")
            
            # æ—¥ä»˜ç¯„å›²è¨­å®šï¼ˆãã®æ—¥ã®00:00:00ã‹ã‚‰23:59:59ã¾ã§ï¼‰
            start_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            end_date = start_date + timedelta(days=1)
            
            self.statistics_processor.set_date_range(start_date, end_date)
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            statistics = await self.statistics_processor.generate_statistics()
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
            report_data = ReportData(
                date=target_date,
                statistics=statistics,
                summary=f"Daily report for {target_date.strftime('%Y-%m-%d')}",
                metadata={
                    'generated_at': datetime.now(timezone.utc).isoformat(),
                    'format': format,
                    'include_statistics': include_statistics
                }
            )
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            result = {}
            
            if format in [ReportFormat.MARKDOWN, ReportFormat.BOTH, ReportFormat.MARKDOWN_ONLY]:
                markdown_report = await self.report_generator.generate_report(report_data)
                result['markdown'] = markdown_report
            
            if format in [ReportFormat.JSON, ReportFormat.BOTH, ReportFormat.JSON_ONLY]:
                result['json'] = {
                    'date': target_date.isoformat(),
                    'statistics': statistics.model_dump(),
                    'summary': report_data.summary
                }
            
            if include_statistics:
                result['statistics'] = statistics
            
            result['metadata'] = report_data.metadata
            
            logger.info(f"Daily report generated successfully for {target_date.date()}")
            return result
            
        except Exception as e:
            logger.error(f"Failed to generate daily report: {e}")
            raise ReportError(f"Failed to generate daily report: {e}")
    
    async def generate_weekly_summary(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """
        é€±æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        
        Args:
            start_date: é–‹å§‹æ—¥
            end_date: çµ‚äº†æ—¥
            
        Returns:
            Dict[str, Any]: é€±æ¬¡ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿
        """
        try:
            logger.info(f"Generating weekly summary from {start_date.date()} to {end_date.date()}")
            
            self.statistics_processor.set_date_range(start_date, end_date)
            
            # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            statistics = await self.statistics_processor.generate_statistics()
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
            report_data = ReportData(
                date=end_date,
                statistics=statistics,
                summary=f"Weekly summary from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}",
                metadata={
                    'type': 'weekly_summary',
                    'period': {
                        'start': start_date.isoformat(),
                        'end': end_date.isoformat()
                    }
                }
            )
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            markdown_report = await self.report_generator.generate_report(report_data)
            
            result = {
                'markdown': markdown_report,
                'statistics': statistics,
                'period': {
                    'start': start_date.isoformat(),
                    'end': end_date.isoformat()
                },
                'metadata': report_data.metadata
            }
            
            logger.info("Weekly summary generated successfully")
            return result
            
        except Exception as e:
            logger.error(f"Failed to generate weekly summary: {e}")
            raise ReportError(f"Failed to generate weekly summary: {e}")
    
    async def schedule_daily_report(self) -> Dict[str, Any]:
        """
        æ—¥å ±ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œ
        
        Returns:
            Dict[str, Any]: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œçµæœ
        """
        try:
            target_date = datetime.now(timezone.utc).replace(hour=0, minute=0, second=0, microsecond=0)
            
            report = await self.generate_daily_report(target_date)
            
            result = {
                'report_id': f"daily_{target_date.strftime('%Y%m%d')}",
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'status': 'success',
                'report': report
            }
            
            logger.info(f"Scheduled daily report completed: {result['report_id']}")
            return result
            
        except Exception as e:
            logger.error(f"Scheduled daily report failed: {e}")
            raise ReportError(f"Scheduled daily report failed: {e}")
    
    async def export_report(
        self,
        report_data: Dict[str, Any],
        filepath: str,
        format: ReportFormat
    ) -> bool:
        """
        ãƒ¬ãƒãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        
        Args:
            report_data: ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
            filepath: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            format: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            
        Returns:
            bool: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæˆåŠŸãƒ•ãƒ©ã‚°
        """
        try:
            path = Path(filepath)
            path.parent.mkdir(parents=True, exist_ok=True)
            
            if format == ReportFormat.MARKDOWN:
                content = report_data.get('markdown', '')
                path.write_text(content, encoding='utf-8')
            
            elif format == ReportFormat.JSON:
                content = {
                    'json': report_data.get('json', {}),
                    'statistics': report_data.get('statistics', {}),
                    'metadata': report_data.get('metadata', {})
                }
                path.write_text(json.dumps(content, indent=2, ensure_ascii=False), encoding='utf-8')
            
            logger.info(f"Report exported successfully to {filepath}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to export report: {e}")
            return False
    
    async def _ensure_database_connection(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šç¢ºèª"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                if await self.db_manager.health_check():
                    return True
                await asyncio.sleep(1)
            except Exception:
                if attempt == max_retries - 1:
                    return False
                await asyncio.sleep(2)
        return False
    
    async def _generate_report_with_retry(self, report_data: ReportData, max_retries: int = 2) -> str:
        """ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        for attempt in range(max_retries + 1):
            try:
                return await self.report_generator.generate_report(report_data)
            except Exception as e:
                if attempt == max_retries:
                    raise
                logger.warning(f"Report generation attempt {attempt + 1} failed: {e}, retrying...")
                await asyncio.sleep(2 ** attempt)  # Exponential backoff


# Factory Functions
def create_report_service(
    db_manager: Optional[DatabaseManager] = None,
    task_manager: Optional[TaskManager] = None,
    settings: Optional[Settings] = None
) -> ReportService:
    """ReportServiceãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼é–¢æ•°"""
    if settings is None:
        settings = get_settings()
    
    if db_manager is None:
        db_manager = get_db_manager()
    
    if task_manager is None:
        from app.tasks.manager import get_task_manager
        task_manager = get_task_manager()
    
    return ReportService(db_manager, task_manager, settings)


# Global service instance (singleton pattern)
_report_service_instance: Optional[ReportService] = None

def get_report_service() -> ReportService:
    """ReportServiceã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""
    global _report_service_instance
    if _report_service_instance is None:
        _report_service_instance = create_report_service()
    return _report_service_instance


def reset_report_service() -> None:
    """ReportServiceã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒªã‚»ãƒƒãƒˆï¼ˆä¸»ã«ãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    global _report_service_instance
    _report_service_instance = None